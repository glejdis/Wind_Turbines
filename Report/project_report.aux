\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{ref1}
\citation{PATEL2016334}
\citation{article}
\citation{dataset}
\citation{dataset}
\citation{chaudhary2020understanding}
\citation{chaudhary2020understanding}
\citation{cochran1967fast}
\citation{FFTTop10}
\citation{maklinFFT}
\citation{chaudhary2020understanding}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Goals}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Materials and Methodology}{1}{section.2}\protected@file@percent }
\newlabel{materials}{{2}{1}{Materials and Methodology}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Dataset Used}{1}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Fast Fourier Transform}{1}{subsection.2.2}\protected@file@percent }
\citation{DSPA}
\citation{DSPA}
\citation{DSPA}
\citation{DSPA}
\citation{FFTTop10}
\citation{10.2307/29775194}
\citation{article}
\citation{xin2016a}
\citation{inproceedings}
\citation{Bajric2015}
\newlabel{DFT}{{1}{2}{Fast Fourier Transform}{equation.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The FFT algorithm breaks the initial problem of length $N$ recursively down into smaller sub problems. The reuse of previous factors leads to computational savings\cite  {DSPA}.\relax }}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{FFT}{{1}{2}{The FFT algorithm breaks the initial problem of length $N$ recursively down into smaller sub problems. The reuse of previous factors leads to computational savings\cite {DSPA}.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Data Segmentation}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Feature Extraction}{2}{subsection.2.4}\protected@file@percent }
\citation{Breiman2001}
\citation{lantz2013machine}
\citation{PATEL2016334}
\citation{burkov2019hundred}
\citation{Breiman2001}
\citation{Breiman2001}
\citation{Alickovic2016MedicalDS}
\citation{Breiman2001}
\citation{lantz2013machine}
\citation{alma990003451080306161}
\citation{yannakakis2018artificial}
\citation{yannakakis2018artificial}
\citation{alma990003451080306161}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Machine Learning Classifiers}{3}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Random Forest Classifier}{3}{subsubsection.2.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Random Forests are ensembles of randomized decision trees. The final classification is done by majority voting.\relax }}{3}{figure.caption.2}\protected@file@percent }
\newlabel{RF}{{2}{3}{Random Forests are ensembles of randomized decision trees. The final classification is done by majority voting.\relax }{figure.caption.2}{}}
\citation{yannakakis2018artificial}
\citation{yannakakis2018artificial}
\citation{Acir}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Acir}
\citation{Acir}
\citation{yannakakis2018artificial}
\citation{yannakakis2018artificial}
\citation{yannakakis2018artificial}
\citation{lantz2013machine}
\citation{yannakakis2018artificial}
\citation{lantz2013machine}
\citation{burkov2019hundred}
\citation{burkov2019hundred}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Support Vector Machine Classifier}{4}{subsubsection.2.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces SVM models maximize the margin between the clusters to be classified. The decision boundary is solely determined by the Support Vectors which are encircled in red.\cite  {yannakakis2018artificial}.\relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{SVM}{{3}{4}{SVM models maximize the margin between the clusters to be classified. The decision boundary is solely determined by the Support Vectors which are encircled in red.\cite {yannakakis2018artificial}.\relax }{figure.caption.3}{}}
\citation{burkov2019hundred}
\citation{burkov2019hundred}
\citation{lantz2013machine}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Visual representation of the architecture of SVM \cite  {Acir}.\relax }}{5}{figure.caption.4}\protected@file@percent }
\newlabel{SVM_2}{{4}{5}{Visual representation of the architecture of SVM \cite {Acir}.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.3}K-means clustering}{5}{subsubsection.2.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The K-means algorithm iteratively searches for $k$ centroids of clusters (here $k=3$). The feature vectors are assigned to one on the centroids according to shortest distance according to a predefined metric. \cite  {burkov2019hundred}.\relax }}{5}{figure.caption.5}\protected@file@percent }
\newlabel{kMeans}{{5}{5}{The K-means algorithm iteratively searches for $k$ centroids of clusters (here $k=3$). The feature vectors are assigned to one on the centroids according to shortest distance according to a predefined metric. \cite {burkov2019hundred}.\relax }{figure.caption.5}{}}
\citation{yannakakis2018artificial}
\citation{burkov2019hundred}
\citation{lantz2013machine}
\citation{lantz2013machine}
\citation{dataset}
\citation{dataset}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.4}Model Performance Assessment}{6}{subsubsection.2.5.4}\protected@file@percent }
\newlabel{modelperf}{{2.5.4}{6}{Model Performance Assessment}{subsubsection.2.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments}{6}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Overall Procedure}{6}{subsection.3.1}\protected@file@percent }
\newlabel{overall procedure}{{3.1}{6}{Overall Procedure}{subsection.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Procedure of analysis on vibration data of wind turbines.\relax }}{7}{figure.caption.6}\protected@file@percent }
\newlabel{proc}{{6}{7}{Procedure of analysis on vibration data of wind turbines.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The exemplary FFT of the 10 \% loading shows a significant difference in the frequency domain between a healthy and a broken gearbox. Especially sensors 1 and 2 show very discriminative behaviour in the frequency domain\relax }}{8}{figure.caption.7}\protected@file@percent }
\newlabel{FFT_10}{{7}{8}{The exemplary FFT of the 10 \% loading shows a significant difference in the frequency domain between a healthy and a broken gearbox. Especially sensors 1 and 2 show very discriminative behaviour in the frequency domain\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The difference in particular frequency domains between the two classes are also apparent in the spectrogram. The spectrogram of sensor 2 for 50\% loading shows the amplitudes between 10 and 15 Hz are significantly increased for the broken gearbox.\relax }}{8}{figure.caption.8}\protected@file@percent }
\newlabel{spectrogram}{{8}{8}{The difference in particular frequency domains between the two classes are also apparent in the spectrogram. The spectrogram of sensor 2 for 50\% loading shows the amplitudes between 10 and 15 Hz are significantly increased for the broken gearbox.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Six minutes of recordings are drawn randomly ten times. The resulting time frames are represented in a highly discriminative two-dimensional feature space.\relax }}{9}{figure.caption.9}\protected@file@percent }
\newlabel{fig43}{{9}{9}{Six minutes of recordings are drawn randomly ten times. The resulting time frames are represented in a highly discriminative two-dimensional feature space.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The entire data recording of 60 minutes in each loading and state (healthy or broken) are represented in a two-dimensional feature space. The data is linearly separable.\relax }}{9}{figure.caption.9}\protected@file@percent }
\newlabel{fig44}{{10}{9}{The entire data recording of 60 minutes in each loading and state (healthy or broken) are represented in a two-dimensional feature space. The data is linearly separable.\relax }{figure.caption.9}{}}
\newlabel{fig45}{{\caption@xref {fig45}{ on input line 254}}{9}{Overall Procedure}{figure.caption.10}{}}
\newlabel{fig48}{{\caption@xref {fig48}{ on input line 270}}{9}{Overall Procedure}{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Twenty time frames are drawn randomly from the overall recording (including all loading) and represented in a two dimensional feature space. We repeat the procedure for time frames of length 15, 20, 30 and 60 seconds. The feature "Mean Sensor 1, bin 2" is highly discriminative. In particular for 60 seconds it provides a good threshold.\relax }}{9}{figure.caption.10}\protected@file@percent }
\newlabel{ThresholdAnalysis}{{11}{9}{Twenty time frames are drawn randomly from the overall recording (including all loading) and represented in a two dimensional feature space. We repeat the procedure for time frames of length 15, 20, 30 and 60 seconds. The feature "Mean Sensor 1, bin 2" is highly discriminative. In particular for 60 seconds it provides a good threshold.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Results}{10}{section.4}\protected@file@percent }
\newlabel{exp}{{4}{10}{Experimental Results}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The SVM model for 2 features is represented graphically. It provides a reliable decision boundary applied on the entire data of 60 minutes.\relax }}{10}{figure.caption.11}\protected@file@percent }
\newlabel{fig50}{{12}{10}{The SVM model for 2 features is represented graphically. It provides a reliable decision boundary applied on the entire data of 60 minutes.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Plot of precision, recall and specificity of the test dataset prediction made by our SVM model trained on mixed dataset. Note that some bins result to dips in the metrics. However, in overall the performance is still very satisfactory.\relax }}{11}{figure.caption.12}\protected@file@percent }
\newlabel{fig51}{{13}{11}{Plot of precision, recall and specificity of the test dataset prediction made by our SVM model trained on mixed dataset. Note that some bins result to dips in the metrics. However, in overall the performance is still very satisfactory.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Plot of precision, recall and specificity of the test dataset prediction made by our SVM model trained on non-mixed dataset. Note that some bins result to dips in the metrics. However, in overall the performance is still similary satisfactory as when using mixed data.\relax }}{11}{figure.caption.13}\protected@file@percent }
\newlabel{fig52}{{14}{11}{Plot of precision, recall and specificity of the test dataset prediction made by our SVM model trained on non-mixed dataset. Note that some bins result to dips in the metrics. However, in overall the performance is still similary satisfactory as when using mixed data.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Confusion Matrix of the SVM model with mixed training data. There is one false negative prediction out of all 6 predictions.\relax }}{12}{figure.caption.14}\protected@file@percent }
\newlabel{fig53}{{15}{12}{Confusion Matrix of the SVM model with mixed training data. There is one false negative prediction out of all 6 predictions.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Confusion Matrix of the SVM model with non-mixed training data.The false negative prediction is similar to the case of mixed data. \relax }}{12}{figure.caption.14}\protected@file@percent }
\newlabel{fig54}{{16}{12}{Confusion Matrix of the SVM model with non-mixed training data.The false negative prediction is similar to the case of mixed data. \relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Cross Validation accuracy and accuracy on the test set for our SVM model with mixed dataset. While for most cases the predictions are accurate, for some bin values the accuracy dip to 50\%. \relax }}{12}{figure.caption.15}\protected@file@percent }
\newlabel{fig55}{{17}{12}{Cross Validation accuracy and accuracy on the test set for our SVM model with mixed dataset. While for most cases the predictions are accurate, for some bin values the accuracy dip to 50\%. \relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Cross Validation accuracy and accuracy on the test set for our SVM model with non-mixed dataset. The plot is again similar to the mixed dataset case, however the dip of accuracy is not as drastic. \relax }}{12}{figure.caption.16}\protected@file@percent }
\newlabel{fig56}{{18}{12}{Cross Validation accuracy and accuracy on the test set for our SVM model with non-mixed dataset. The plot is again similar to the mixed dataset case, however the dip of accuracy is not as drastic. \relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Receiver operating characteristics (ROC) for each bin split for SVM model. This shows that the SVM model prediction is very accurate for most bin values as the curves tend towards the upper left corner of the diagram.\relax }}{13}{figure.caption.17}\protected@file@percent }
\newlabel{fig57}{{19}{13}{Receiver operating characteristics (ROC) for each bin split for SVM model. This shows that the SVM model prediction is very accurate for most bin values as the curves tend towards the upper left corner of the diagram.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Area Under the Curve for different bin sizes SVM classifier. The plot confirms that in overall, the SVM model provides good predictions for our dataset with the exception of a few bin values. This provides an interesting insight into the behaviour of our time-series data and how splitting the data affects the data and performance.\relax }}{13}{figure.caption.18}\protected@file@percent }
\newlabel{fig58}{{20}{13}{Area Under the Curve for different bin sizes SVM classifier. The plot confirms that in overall, the SVM model provides good predictions for our dataset with the exception of a few bin values. This provides an interesting insight into the behaviour of our time-series data and how splitting the data affects the data and performance.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces The overall random forest classifier as given by MATLAB. The averaged decision tree would be a single node decision tree, whereby the splitting decision is taken to be the value of $x_1$, which is the mean of sensor 1. The input would then be classified as either 'broken' (1) or 'healthy' (0).\relax }}{14}{figure.caption.19}\protected@file@percent }
\newlabel{fig59}{{21}{14}{The overall random forest classifier as given by MATLAB. The averaged decision tree would be a single node decision tree, whereby the splitting decision is taken to be the value of $x_1$, which is the mean of sensor 1. The input would then be classified as either 'broken' (1) or 'healthy' (0).\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Scatter plot of Random Forest classifier. This visualization further confirms the splitting decision taken by the overall random forest classifier for our dataset. By visual inspection one can observed that the dataset can be classified from the mean of sensor 1. \relax }}{14}{figure.caption.20}\protected@file@percent }
\newlabel{fig60}{{22}{14}{Scatter plot of Random Forest classifier. This visualization further confirms the splitting decision taken by the overall random forest classifier for our dataset. By visual inspection one can observed that the dataset can be classified from the mean of sensor 1. \relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Plot of precision, recall and specificity of the test dataset prediction of our Random Forest trained with mixed dataset. This also shows that the random forest model performs relatively well when trained with binned and mixed dataset, with the exceptions for a few bin values where the metric values dip. \relax }}{15}{figure.caption.21}\protected@file@percent }
\newlabel{fig61}{{23}{15}{Plot of precision, recall and specificity of the test dataset prediction of our Random Forest trained with mixed dataset. This also shows that the random forest model performs relatively well when trained with binned and mixed dataset, with the exceptions for a few bin values where the metric values dip. \relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Plot of precision, recall and specificity of the test dataset prediction of our Random Forest trained with non-mixed dataset. The metrics look similar to the mixed dataset case.\relax }}{15}{figure.caption.22}\protected@file@percent }
\newlabel{fig62}{{24}{15}{Plot of precision, recall and specificity of the test dataset prediction of our Random Forest trained with non-mixed dataset. The metrics look similar to the mixed dataset case.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Confusion Matrix of the random forest model with mixed training data split into 40 bins. There is one false negative prediction out of all 6 predictions, which is comparable to the SVM model performance.\relax }}{15}{figure.caption.23}\protected@file@percent }
\newlabel{fig63}{{25}{15}{Confusion Matrix of the random forest model with mixed training data split into 40 bins. There is one false negative prediction out of all 6 predictions, which is comparable to the SVM model performance.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Confusion Matrix of the random forest model with non-mixed training data split into 40 bins. There is one false negative prediction out of all 6 predictions, similar to the case of training with mixed training data.\relax }}{15}{figure.caption.23}\protected@file@percent }
\newlabel{fig64}{{26}{15}{Confusion Matrix of the random forest model with non-mixed training data split into 40 bins. There is one false negative prediction out of all 6 predictions, similar to the case of training with mixed training data.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Cross Validation accuracy of the random forest model training on non-mixed dataset. The accuracy values dip slightly for some bin values but overall the model shows very high accuracy.\relax }}{16}{figure.caption.24}\protected@file@percent }
\newlabel{fig65}{{27}{16}{Cross Validation accuracy of the random forest model training on non-mixed dataset. The accuracy values dip slightly for some bin values but overall the model shows very high accuracy.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Cross Validation accuracy of the random forest model training on mixed dataset. The performance again is very similar to the case of the model trained on non-mixed dataset.\relax }}{16}{figure.caption.25}\protected@file@percent }
\newlabel{fig66}{{28}{16}{Cross Validation accuracy of the random forest model training on mixed dataset. The performance again is very similar to the case of the model trained on non-mixed dataset.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces A visualization of k-means clustering applied to our dataset. For the whole dataset, one can see that the k-means clustering cluster the dataset easily as the distance between healthy and broken datasets are very far apart.\relax }}{17}{figure.caption.26}\protected@file@percent }
\newlabel{fig67}{{29}{17}{A visualization of k-means clustering applied to our dataset. For the whole dataset, one can see that the k-means clustering cluster the dataset easily as the distance between healthy and broken datasets are very far apart.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Plot of precision, recall and specificity of the test dataset classification made by k-Means clustering on mixed dataset. We can observe that the k-Means performs very poorly in general when the dataset are split into various number of bins.\relax }}{17}{figure.caption.27}\protected@file@percent }
\newlabel{fig68}{{30}{17}{Plot of precision, recall and specificity of the test dataset classification made by k-Means clustering on mixed dataset. We can observe that the k-Means performs very poorly in general when the dataset are split into various number of bins.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Plot of precision, recall and specificity of the test dataset classification made by k-Means clustering on non-mixed dataset. Similary, k-Means performs very poorly as in the case with mixed dataset.\relax }}{17}{figure.caption.28}\protected@file@percent }
\newlabel{fig69}{{31}{17}{Plot of precision, recall and specificity of the test dataset classification made by k-Means clustering on non-mixed dataset. Similary, k-Means performs very poorly as in the case with mixed dataset.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Confusion Matrix of the k-Means classifier on mixed dataset split into 40 bins. We can observe that k-Means clustering performs worse than SVM and random forest, whereby 2 out of the 6 classifications are false negatives.\relax }}{18}{figure.caption.29}\protected@file@percent }
\newlabel{fig70}{{32}{18}{Confusion Matrix of the k-Means classifier on mixed dataset split into 40 bins. We can observe that k-Means clustering performs worse than SVM and random forest, whereby 2 out of the 6 classifications are false negatives.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Confusion Matrix of the k-Means classifier on non-mixed dataset split into 40 bins. We can observe that k-Means clustering performs worse than SVM and random forest, similar again to the mixed dataset case.\relax }}{18}{figure.caption.29}\protected@file@percent }
\newlabel{fig71}{{33}{18}{Confusion Matrix of the k-Means classifier on non-mixed dataset split into 40 bins. We can observe that k-Means clustering performs worse than SVM and random forest, similar again to the mixed dataset case.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Classification accuracy of K-means clustering on mixed test dataset. In overall, the accuracy plot also shows how poor the k-means clustering is for binned datasets. \relax }}{18}{figure.caption.30}\protected@file@percent }
\newlabel{fig72}{{34}{18}{Classification accuracy of K-means clustering on mixed test dataset. In overall, the accuracy plot also shows how poor the k-means clustering is for binned datasets. \relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Classification accuracy of K-means clustering on non-mixed test dataset. The same poor accuracy and performance can also be observed when the clustering is done on non-mixed test dataset.\relax }}{18}{figure.caption.31}\protected@file@percent }
\newlabel{fig73}{{35}{18}{Classification accuracy of K-means clustering on non-mixed test dataset. The same poor accuracy and performance can also be observed when the clustering is done on non-mixed test dataset.\relax }{figure.caption.31}{}}
\newlabel{fig45}{{\caption@xref {fig45}{ on input line 508}}{19}{Experimental Results}{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Twenty randomly drawn time samples of 60 seconds have been drawn from all loadings and states (400 snippets). They were used to visually identify a the threshold of 0.244 (Training). Eight distinct samples of 60 seconds are used to test the threshold on unknown data.\relax }}{19}{figure.caption.32}\protected@file@percent }
\newlabel{ThresholdRes}{{36}{19}{Twenty randomly drawn time samples of 60 seconds have been drawn from all loadings and states (400 snippets). They were used to visually identify a the threshold of 0.244 (Training). Eight distinct samples of 60 seconds are used to test the threshold on unknown data.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces The results of the threshold are satisfactory. In the balanced test set of 160 samples there are 2 false-negatives leading to a slight decrease in recall and accuracy. Precision and specificity are maximal.\relax }}{19}{figure.caption.33}\protected@file@percent }
\newlabel{ThresEval}{{37}{19}{The results of the threshold are satisfactory. In the balanced test set of 160 samples there are 2 false-negatives leading to a slight decrease in recall and accuracy. Precision and specificity are maximal.\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{20}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{20}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Future Work}{20}{section.7}\protected@file@percent }
\bibstyle{abbrv}
\bibdata{references}
\bibcite{Acir}{1}
\bibcite{Alickovic2016MedicalDS}{2}
\bibcite{Bajric2015}{3}
\bibcite{Breiman2001}{4}
\bibcite{burkov2019hundred}{5}
\bibcite{chaudhary2020understanding}{6}
\bibcite{cochran1967fast}{7}
\bibcite{Goodfellow-et-al-2016}{8}
\bibcite{lantz2013machine}{9}
\bibcite{maklinFFT}{10}
\bibcite{alma990003451080306161}{11}
\bibcite{dataset}{12}
\bibcite{PATEL2016334}{13}
\bibcite{article}{14}
\bibcite{DSPA}{15}
\bibcite{FFTTop10}{16}
\bibcite{inproceedings}{17}
\bibcite{10.2307/29775194}{18}
\bibcite{ref1}{19}
\bibcite{xin2016a}{20}
\bibcite{yannakakis2018artificial}{21}
\gdef \@abspage@last{22}
